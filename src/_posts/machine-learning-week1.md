---
title: courseraの機械学習の勉強記録 〜第一週〜
tags: machine-learning
date: "2022-03-30"
isPublic: false
coverImage: "/cover/make-blog/index.jpeg"
---

## イントロダクション

### 教師あり学習

元々のデータセットを渡し、そこから学習させる方法

特徴からデータを分けて、その特徴に沿ってデータを分けて、傾向を学習させる

- 回帰問題
  - 連続値などの値の予測
    - ある面積の住宅の売却値はいくらか？
- 分類問題
  - データが属するクラスを予想する
    - 例) 腫瘍が悪性か陽性か

### 教師なし学習

クラスタリング・アルゴリズムという手法
→ データ間の類似度に基づいて、データをグループ分けする手法

- カクテルパーティー問題

2 つのマイクロフォンを違う位置に設置して、別の 2 つの音声をマイクロフォンに拾わせると、より小さく聞こえた方を除去してくれる。

## 線形回帰

そもそも線形回帰とは

> 説明変数（独立変数ともいう）に対して目的変数（従属変数、あるいは反応変数ともいう）が線形またはそれから近い値で表される状態。(Wikipedia)

基本的に x が独立変数、y が従属変数となる。

例えば、x を住宅の面積、y を住宅価格として、面積がいくつの時に価格がいくらになるのかを求める。
この時、与えられるデータセットを、「トレーニングセット」と言う。
そして、予測するのに使う仮説の数式は、`hθ(x) = θ0 + θ1x`となる。
θ0 や θ1 は、モデルのパラメータと言う。
このパラメータをどうやって求めるかということをこのセッションでは行なっている。

ここでやりたいことは、最小の θ0 と θ1 を求めることで、これは、
`(hθ(x(i))-y(i))^2`の値を小さくすること。

最終的な目的関数
`J(θ0,θ1) = 1/2m Σm, i=1(hθ(x(i)) – y(i))²`
これは、二乗誤差関数と呼ばれたりする。
